---
title: Pagination
---

GraphQL servers provide selective access to a sprawling graph of data that GraphQL clients might request, containing countless entity objects and their fields. A typical GraphQL client needs only a small subgraph of this complete data graph, and uses GraphQL queries to fetch all and only the fields it needs.

While requesting the specific fields your application needs is an important technique for preventing over-fetching, it's not the only way to reduce response size. A single list field can hold arbitrarily many elements, possibly infinitely many. If you need some of that data but not all of it, what can you do? Omitting the field from your query is not an option, but neither is requesting the entire list.

To solve this problem, GraphQL servers often let clients avoid fetching entire lists of data at once, by allowing the lists to be split up into separate chunks or _pages_, which can be requested incrementally using field arguments like `offset`, `cursor`, and/or `limit`.

_Pagination_ is the term for this general technique of incrementally requesting large collections of data (usually lists) in smaller pages, using field arguments. Pagination comes in a number of different flavors: offset-based, cursor-based, page-number-based, forwards, backwards, displayed in discrete pages or via infinite scroll, and so on, each calling for a slightly different set of field arguments. As this diversity suggests, pagination is not an automatic feature of list-valued fields in GraphQL, but GraphQL server tools make pagination straightforward to set up, and so it has become a ubiquitous pattern.

Rather than attempting to standardize on one style of pagination to serve all use cases, Apollo Client enables any and all such patterns through the flexible `InMemoryCache` field policy API, which  allows you to configure the client-side behavior of fields across your graph in a consistent, declarative, reusable way, in one place, rather than reproducing custom field logic wherever the field is used throughout your application.

You can get started by copying and pasting example code from this documentation, but you will no doubt end up adapting and expanding your field policies to suit your specific needs. Whenever you need to apply similar logic elsewhere, remember that you can encapsulate common policies in reusable policy-generating functions.

## Combining pages with `merge` functions

While it is possible to store individual pages of list data separately on the client, application code that consumes the data tends to be simpler if the client combines all the pages it has received so far into a single list, using field arguments to guide how new data is spliced together with existing data.

Suppose your schema defines the following types:
```gql
extend type Query {
  feed(
    type: FeedType!,
    offset: Int,
    limit: Int,
  ): [FeedItem!]
}

enum FeedType {
  PERSONAL
  PUBLIC
}

type FeedItem {
  id: String!
  # ...
}
```

The query you use to fetch data for the `Query.feed` field might look something like this:
```ts
const FEED_QUERY = gql`
  query Feed($type: FeedType!, $offset: Int, $limit: Int) {
    feed(type: $type, offset: $offset, limit: $limit) {
      id
      # ... other FeedItem fields
    }
  }
`;
```

Without input from you, `InMemoryCache` has no way of knowing how to interpret these field arguments, so it must assume they are all important. For example, imagine writing two consecutive pages into a cache that does not know about your pagination system:
```ts
const cache = new InMemoryCache;

cache.writeQuery({
  query: FEED_QUERY,
  variables: {
    type: "PERSONAL",
    offset: 0,
    limit: 2,
  },
  data: {
    feed: [
      { __typename: "FeedItem", id: 1 },
      { __typename: "FeedItem", id: 2 },
    ]
  }
});

cache.writeQuery({
  query: FEED_QUERY,
  variables: {
    type: "PERSONAL",
    offset: 2, // Changed!
    limit: 2,
  },
  data: {
    feed: [
      { __typename: "FeedItem", id: 3 },
      { __typename: "FeedItem", id: 4 },
    ]
  }
});
```
The cache stores those two results separately by default, using keys derived from both the field name (`feed`) and the serialized arguments (`limit`, `offset`, and `type`):
```js
expect(cache.extract()).toEqual({
  // Internal, normalized InMemoryCache data:
  "FeedItem:1": { __typename: "FeedItem", id: 1 },
  "FeedItem:2": { __typename: "FeedItem", id: 2 },
  "FeedItem:3": { __typename: "FeedItem", id: 3 },
  "FeedItem:4": { __typename: "FeedItem", id: 4 },
  "ROOT_QUERY": {
    __typename: "Query",
    // Notice we end up with two separate pages, one per unique
    // combination of Query.feed field arguments.
    'feed({"limit":2,"offset":0,"type":"PERSONAL"})': [
      { __ref: "FeedItem:1" },
      { __ref: "FeedItem:2" },
    ],
    'feed({"limit":2,"offset":2,"type":"PERSONAL"})': [
      { __ref: "FeedItem:3" },
      { __ref: "FeedItem:4" },
    ],
  },
});
```
A simplistic way to accumulate a single list instead of storing separate lists is to concatenate the results together, using a `merge` function for the `Query.feed` field:
```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          // Stop using limit and offset to differentiate field keys,
          // but continue using type.
          keyArgs: ["type"],
          // If there is no existing value for this field, existing
          // will be undefined.
          merge(existing = [], incoming) {
            return [...existing, ...incoming];
          },
        }
      }
    }
  }
})
```
Now that the cache has a strategy for combining incoming pages with existing data, its internal representation will be a bit more compact:
```js
expect(cache.extract()).toEqual({
  "FeedItem:1": { __typename: "FeedItem", id: 1 },
  "FeedItem:2": { __typename: "FeedItem", id: 2 },
  "FeedItem:3": { __typename: "FeedItem", id: 3 },
  "FeedItem:4": { __typename: "FeedItem", id: 4 },
  "ROOT_QUERY": {
    __typename: "Query",
    // One consolidated list of Query.feed data (per type):
    'feed({"type":"PERSONAL"})': [
      { __ref: "FeedItem:1" },
      { __ref: "FeedItem:2" },
      { __ref: "FeedItem:3" },
      { __ref: "FeedItem:4" },
    ],
  },
});
```
However, this simple strategy makes some risky assumptions about the order of the written pages, because it ignores the `offset` and `limit` arguments. In order to handle those arguments correctly, your `merge` function should use `options.args` to decide where to put the incoming data:
```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          keyArgs: ["type"],
          merge(existing, incoming, { args: { offset = 0 }}) {
            const merged = existing ? existing.slice(0) : [];
            for (let i = 0; i < incoming.length; ++i) {
              merged[offset + i] = incoming[i];
            }
            return merged;
          },
        },
      },
    },
  },
});
```
This logic handles sequential page writes the same way the concatenation strategy would, but it can also tolerate repeated, overlapping, or out-of-order writes, without duplicating any list items.

As we mentioned in the introduction above, common field policy patterns can be abstracted away into helper functions, such as the `offsetLimitPagination` function that `@apollo/client/utilities` provides:
```js
import { offsetLimitPagination } from "@apollo/client/utilities"

const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: offsetLimitPagination(["type"]),
      },
    },
  },
});
```

## Two kinds of `read` function

Now that you understand the basics of `merge` functions, you may be wondering about the other direction cache data can flow: what happens when you read from a paginated field?

Just as a `merge` function uses field arguments to guide how paginated data is reassembled from individual pages, maintaining a single combined list of internal cache data, a custom `read` function has the opportunity to translate that internal list back into individual pages, again using field arguments to select the page in question, and possibly even sort or filter the data, depending on what other arguments you choose to implement. This capability goes beyond returning the same pages that were originally received, since a `read` function for `offset`/`limit` pagination could read from any available `offset`, with any desired `limit`:
```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          read(existing, { args: { offset, limit }}) {
            // Important to return undefined when existing is undefined,
            // rather than defaulting to an empty array, because returning
            // undefined is how a read function signals that the field is
            // missing, causing the query to be fetched from the network.
            return existing && existing.slice(offset, offset + limit);
          },

          // The keyArgs and merge configurations are the same as above.
          keyArgs: ["type"],
          merge(existing, incoming, { args: { offset = 0 }}) {
            const merged = existing ? existing.slice(0) : [];
            for (let i = 0; i < incoming.length; ++i) {
              merged[offset + i] = incoming[i];
            }
            return merged;
          },
        },
      },
    },
  },
});
```

Depending on the assumptions you feel comfortable making, you may wish to make this code more defensive. For example, you might want to provide default values for the `offset` and `limit` arguments, in case someone tries to access the `Query.feed` field without using the appropriate arguments:
```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          read(existing, {
            args: {
              // Default to returning the whole array, if the offset and
              // limit arguments are not provided.
              offset = 0,
              limit = existing?.length,
            } = {},
          }) {
            return existing && existing.slice(offset, offset + limit);
          },
          // ... keyArgs, merge ...
        },
      },
    },
  },
});
```

This style of `read` function, which takes responsibility for re-paginating your data based on the field arguments, essentially inverts the behavior of the `merge` function, so your application can query different pages using different arguments. **However, this is not the only way to write a `read` function!**

Another reasonable approach to writing a `read` function for a paginated field like `Query.feed` is to _ignore_ the `offset` and `limit` arguments, and always return the entire list, so your application code can take responsibility for slicing the list into pages, depending on how you want to display the data in different parts of the application.

If you adopt this second approach, you may discover you don't even need a `read` function, because you no longer need to examine the `args` or slice the `existing` data before returning it. That's why the `offsetLimitPagination` helper we mentioned above is implemented without a `read` function.

> When you provide both a `merge` function and `read` function, `keyArgs: false` will be assumed by default (though it can be overridden to something else, like `keyArgs: ["type"]`). However, if you provide only a `merge` function (or only a `read` function), you should specify `keyArgs` explicitly, to override the default behavior of considering all arguments relevant.

Which approach is right for your application? While the answer may vary from field to field, the tradeoffs between these two approaches should become clearer once we introduce Apollo Client's primary pagination API: `fetchMore`.

## Using `fetchMore`

In Apollo, the easiest way to request additional pages of paginated data is a function called [`fetchMore`](../caching/advanced-topics/#incremental-loading-fetchmore), which is a method of the `ObservableQuery` class returned by `client.watchQuery`, and is also included in the object returned by the `useQuery` Hook.

In the `options` you pass to `fetchMore`, you specify the new `options.variables` and (optionally) an `options.query` to be used when fetching the additional data (if no `options.query` is specified, the original query passed to `client.watchQuery` or `useQuery` will be used).

In Apollo Client 2, you would also provide an `options.updateQuery` function, which was responsible for merging fetched pages with existing data in the cache. In Apollo Client 3, custom `merge` functions allow this logic to be specified in a central location, rather than duplicated everywhere you call `fetchMore`.

## Offset-based

We've already seen `offset`-based pagination in the examples above. This style of pagination works well for immutable lists, or lists whose element positions are not expected to change, since moving or removing elements could alter the offsets of the elements in the list, sometimes causing elements to be skipped or duplicated if the list is modified on the server between page requests.

Although offset-based pagination has its shortcomings, it is a common pattern found in many applications, in part because it is straightforward to implement on the backend. In SQL, for example, numbered pages can easily be generated by using [OFFSET and LIMIT](https://www.postgresql.org/docs/8.2/static/queries-limit.html).

If you configure the `Query.feed` field policy using our `offsetLimitPagination` helper imported from `@apollo/client/utilities`, then you can use `fetchMore` in with `useQuery` like so:

```jsx
const FeedData({ type = "PUBLIC" }) {
  const { loading, data, fetchMore } = useQuery(FEED_QUERY, {
    variables: {
      type: type.toUpperCase(),
      offset: 0,
      limit: 10
    },
  });

  // If you want your component to rerender with loading:true whenever
  // fetchMore is called, add notifyOnNetworkStatusChange:true to the
  // options you pass to useQuery above.
  if (loading) return <Loading/>;

  return (
    <Feed
      entries={data.feed || []}
      onLoadMore={() => fetchMore({
        variables: {
          offset: data.feed.length
        },
      })}
    />
  );
}
```

As you can see, `fetchMore` is accessible through the `useQuery` Hook result object. By default, `fetchMore` will use the original `query` and `variables`, so we only need to pass the variable that is changing: the `offset`. Once the new data is returned from the server, it will be automatically merged with any existing `Query.feed` data in the cache, which will cause `useQuery` to rerender with the expanded list of data.

This style of `fetchMore` usage assumes you want your component to receive the entire available list each time it renders, containing data from all pages received so far. This is the second kind of `read` function discussed in the [Two kinds of `read` function](#two-kinds-of-read-function) section above.

If you are using a `Query.feed` field policy containing a `read` function that uses `args.offset` and `args.limit` to return a single page of data (the first kind of `read` function discussed above), the code above will still work, but you may be surprised that your component does not automatically rerender with additional data (beyond the first page) after `fetchMore` finishes. This happens because the original `variables: { offset: 0, limit: 10 }` are still in effect, and the first 10 items were not changed by the `fetchMore` call, so your `read` function returns the same page as before.

Before you can fix this problem, you first need to think about the behavior that you want. Should your component continue displaying only the first page, or should it now display the page we just received, or should it display the entire list of known data? Regardless of which option you prefer, these alternatives all boil down to the `variables` you pass to `useQuery`, which must change if you want your component to render different data.

For example, to display all the data received so far, you could modify the previous example as follows:
```jsx
const FeedData({ type = "PUBLIC" }) {
  const [limit, setLimit] = useState(10);
  const { loading, data, fetchMore } = useQuery(FEED_QUERY, {
    variables: {
      type: type.toUpperCase(),
      offset: 0,
      limit,
    },
  });

  if (loading) return <Loading/>;

  return (
    <Feed
      entries={data.feed || []}
      onLoadMore={() => {
        const currentLength = data.feed.length;
        fetchMore({
          variables: {
            offset: currentLength,
            limit: 10,
          },
        }).then(fetchMoreResult => {
          // Update variables.limit for the original query to include
          // the newly added feed items.
          setLimit(currentLength + fetchMoreResult.data.feed.length);
        });
      }
    />
  );
}
```
This code uses a React `useState` Hook to store the current `limit` value, which it updates by calling `setLimit` in a callback attached to the `Promise` returned by `fetchMore`.

You could store `offset` in a React `useState` Hook as well, if you need the `offset` to change. Exactly when and how these `variables` change is up to your component, and may not always be the result of calling `fetchMore`, so it makes sense to use React component state to store these variable values.

> If you are not using React and `useQuery`, the `ObservableQuery` object returned by `client.watchQuery` has a method called `setVariables` that you can call to update the original variables.

Because `fetchMore` requires some extra work to update the original variables if you're using a `read` function that is sensitive to those variables (the second kind of `read` function), it's fair to say `fetchMore` encourages the simpler first kind of `read` function, which simply returns all available data.

However, now that you understand your options, there's nothing wrong with moving read-time pagination logic out of your application code and into your field `read` functions. Both kinds of `read` functions have their uses, and both can be made to work with `fetchMore`.

## Using list element IDs as cursors

Since numeric offsets within paginated lists can be unreliable, a common improvement is to identify the beginning of a page using some unique identifier that belongs to each element of the list.

If the list represents a set of elements without duplicates, this identifier could simply be the unique ID of each object, allowing additional pages to be requested using the ID of the last object in the list, together with some `limit` argument. With this approach, the requested `cursor` ID should not appear in the new page, since it identifies the item just before the beginning of the page.

Since the elements of the list could be normalized `Reference` objects, you will probably want to use the `options.readField` helper function to read the `id` field in your `merge` and/or `read` functions:
```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          keyArgs: ["type"],

          merge(existing, incoming, {
            args: { cursor },
            readField,
          }) {
            const merged = existing ? existing.slice(0) : [];
            let offset = offsetFromCursor(merged, cursor, readField);
            // If we couldn't find the cursor, default to appending to
            // the end of the list, so we don't lose any data.
            if (offset < 0) offset = merged.length;
            // Now that we have a reliable offset, the rest of this logic
            // is the same as in offsetLimitPagination.
            for (let i = 0; i < incoming.length; ++i) {
              merged[offset + i] = incoming[i];
            }
            return merged;
          },

          // If you always want to return the whole list, you can omit
          // this read function.
          read(existing, {
            args: { cursor, limit = existing.length },
            readField,
          }) {
            if (existing) {
              let offset = offsetFromCursor(existing, cursor, readField);
              // If we couldn't find the cursor, default to reading the
              // entire list.
              if (offset < 0) offset = 0;
              return existing.slice(offset, offset + limit);
            }
          },
        },
      },
    },
  },
});

function offsetFromCursor(items, cursor, readField) {
  // Search from the back of the list because the cursor we're
  // looking for is typically the ID of the last item.
  for (let i = items.length - 1; i >= 0; --i) {
    const item = items[i];
    // Using readField works for both non-normalized objects
    // (returning item.id) and normalized references (returning
    // the id field from the referenced entity object), so it's
    // a good idea to use readField when you're not sure what
    // kind of elements you're dealing with.
    if (readField("id", item) === cursor) {
      // Add one because the cursor identifies the item just
      // before the first item in the page we care about.
      return i + 1;
    }
  }
  // Report that the cursor could not be found.
  return -1;
}
```

Since items can be removed from, added to, or moved around within the list without altering their `id` fields, this pagination strategy tends to be more resilient to list mutations than the offset-based strategy we saw above.

However, this strategy works best when your `merge` function always appends new pages to the existing data, since it doesn't take any precautions to avoid overwriting elements if the `cursor` falls somewhere in the middle of the existing data.

## Cursor-based

More generally, any unique identifier that allows indexing into the list can be used as a _cursor_. In cases where the list could have duplicates, or is sorted or filtered according to some criteria, the cursor may need to encode not just a position within the list but also the sorting/filtering logic that produced the list. In such situations, since the cursor does not logically belong to the elements of the list, the cursor may be returned separately from the list:

```jsx
const MORE_COMMENTS_QUERY = gql`
  query MoreComments($cursor: String) {
    moreComments(cursor: $cursor) {
      cursor # corresponds to the end of the list
      comments {
        author
        text
      }
    }
  }
`;
```

```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          keyArgs: ["type"],
          merge(existing, incoming, {
            args: { cursor, limit },
          }) {
            const merged = existing ? existing.slice(0) : [];

            merged.findIndex()

            for (let i = 0; i < incoming.length; ++i) {
              merged[offset + i] = incoming[i];
            }
            return merged;
          },
        },
      },
    },
  },
});
```

In cursor-based pagination, a "cursor" is used to keep track of where in the data set the next items should be fetched from. Sometimes the cursor can be quite simple and just refer to the ID of the last object fetched, but in some cases — for example lists sorted according to some criteria — the cursor needs to encode the sorting criteria in addition to the ID of the last object fetched.

Implementing cursor-based pagination on the client isn't all that different from offset-based pagination, but instead of using an absolute offset, we keep a reference to the last object fetched and information about the sort order used.

In the example below, we use a `fetchMore` query to continuously load new comments, which will be prepended to the list. The cursor to be used in the `fetchMore` query is provided in the initial server response, and is updated whenever more data is fetched.

```jsx
const MORE_COMMENTS_QUERY = gql`
  query MoreComments($cursor: String) {
    moreComments(cursor: $cursor) {
      cursor
      comments {
        author
        text
      }
    }
  }
`;

function CommentsWithData() {
  const { data: { moreComments: {comments, cursor} }, loading, fetchMore } = useQuery(
    MORE_COMMENTS_QUERY
  );

  return (
    <Comments
      entries={comments || []}
      onLoadMore={() =>
        fetchMore({
          // note this is a different query than the one used in the
          // Query component
          query: MORE_COMMENTS_QUERY,
          variables: { cursor: cursor },
          updateQuery: (previousResult, { fetchMoreResult }) => {
            const previousEntry = previousResult.entry;
            const newComments = fetchMoreResult.moreComments.comments;
            const newCursor = fetchMoreResult.moreComments.cursor;

            return {
              // By returning `cursor` here, we update the `fetchMore` function
              // to the new cursor.
              cursor: newCursor,
              entry: {
                // Put the new comments in the front of the list
                comments: [...newComments, ...previousEntry.comments]
              },
              __typename: previousEntry.__typename
            };
          }
        })
      }
    />
  );
}
```

## Relay-style cursor pagination

Relay, another popular GraphQL client, is opinionated about the input and output of paginated queries, so people sometimes build their server's pagination model around Relay's needs. If you have a server that is designed to work with the [Relay Cursor Connections](https://facebook.github.io/relay/graphql/connections.htm) spec, you can also call that server from Apollo Client with no problems.

Using Relay-style cursors is very similar to basic cursor-based pagination.  The main difference is in the format of the query response which affects where you get the cursor.

Relay provides a `pageInfo` object on the returned cursor connection which contains the cursor of the first and last items returned as the properties `startCursor` and `endCursor` respectively.  This object also contains a boolean property `hasNextPage` which can be used to determine if there are more results available.

The following example specifies a request of 10 items at a time and that results should start after the provided `cursor`.  If `null` is passed for the cursor relay will ignore it and provide results starting from the beginning of the data set which allows the use of the same query for both initial and subsequent requests.

```jsx
const COMMENTS_QUERY = gql`
  query Comments($cursor: String) {
    Comments(first: 10, after: $cursor) {
      edges {
        node {
          author
          text
        }
      }
      pageInfo {
        endCursor
        hasNextPage
      }
    }
  }
`;

function CommentsWithData() {
  const { data: { Comments: comments }, loading, fetchMore } = useQuery(
    COMMENTS_QUERY
  );

  return (
    <Comments
      entries={comments || []}
      onLoadMore={() =>
        fetchMore({
          variables: {
            cursor: comments.pageInfo.endCursor
          },
          updateQuery: (previousResult, { fetchMoreResult }) => {
            const newEdges = fetchMoreResult.comments.edges;
            const pageInfo = fetchMoreResult.comments.pageInfo;

            return newEdges.length
              ? {
                  // Put the new comments at the end of the list and update `pageInfo`
                  // so we have the new `endCursor` and `hasNextPage` values
                  comments: {
                    __typename: previousResult.comments.__typename,
                    edges: [...previousResult.comments.edges, ...newEdges],
                    pageInfo
                  }
                }
              : previousResult;
          }
        })
      }
    />
  );
}
```

## The `@connection` directive

When using paginated queries, results from accumulated queries can be hard to find in the store, as the parameters passed to the query are used to determine the default store key but are usually not known outside the piece of code that executes the query. This is problematic for imperative store updates, as there is no stable store key for updates to target. To direct Apollo Client to use a stable store key for paginated queries, you can use the optional `@connection` directive to specify a store key for parts of your queries. For example, if we wanted to have a stable store key for the feed query earlier, we could adjust our query to use the `@connection` directive:

```js
const FEED_QUERY = gql`
  query Feed($type: FeedType!, $offset: Int, $limit: Int) {
    currentUser {
      login
    }
    feed(type: $type, offset: $offset, limit: $limit) @connection(key: "feed", filter: ["type"]) {
      id
      # ...
    }
  }
`;
```

This would result in the accumulated feed in every query or `fetchMore` being placed in the store under the `feed` key, which we could later use for imperative store updates. In this example, we also use the `@connection` directive's optional `filter` argument, which allows us to include some arguments of the query in the store key. In this case, we want to include the `type` query argument in the store key, which results in multiple store values that accumulate pages from each type of feed.
